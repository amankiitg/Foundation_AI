{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amankiitg/Foundation_AI/blob/main/GenAI_Lecture_4_RNN_Translator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: RNN Translator coded from scratch"
      ],
      "metadata": {
        "id": "T2rASIA7Fkef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# --------------------------\n",
        "# 1. A Tiny Manual RNN Encoder\n",
        "# --------------------------\n",
        "class TinyEncoder(nn.Module):\n",
        "    def __init__(self, input_vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_vocab_size, embed_size)\n",
        "\n",
        "        # RNN parameters\n",
        "        self.hidden_size = hidden_size\n",
        "        self.W_h = nn.Parameter(torch.randn(hidden_size, hidden_size)*0.1)\n",
        "        self.W_x = nn.Parameter(torch.randn(hidden_size, embed_size)*0.1)\n",
        "        self.b   = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "    def forward(self, src_tokens):\n",
        "        \"\"\"\n",
        "        src_tokens: shape (src_len,)\n",
        "        Returns final hidden state (hidden_size,).\n",
        "        \"\"\"\n",
        "        h = torch.zeros(self.hidden_size)\n",
        "\n",
        "        for t in range(src_tokens.shape[0]):\n",
        "            token_id = src_tokens[t]\n",
        "            x_t = self.embedding(token_id)\n",
        "\n",
        "            h = torch.tanh(\n",
        "                torch.mv(self.W_h, h) +\n",
        "                torch.mv(self.W_x, x_t) +\n",
        "                self.b\n",
        "            )\n",
        "\n",
        "        return h\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# 2. A Tiny Manual RNN Decoder\n",
        "# -------------------------\n",
        "class TinyDecoder(nn.Module):\n",
        "    def __init__(self, output_vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(output_vocab_size, embed_size)\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.W_h = nn.Parameter(torch.randn(hidden_size, hidden_size)*0.1)\n",
        "        self.W_x = nn.Parameter(torch.randn(hidden_size, embed_size)*0.1)\n",
        "        self.b   = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        # Output projection\n",
        "        self.W_out = nn.Parameter(torch.randn(output_vocab_size, hidden_size)*0.1)\n",
        "        self.b_out = nn.Parameter(torch.zeros(output_vocab_size))\n",
        "\n",
        "    def forward(self, dec_tokens, init_hidden):\n",
        "        h = init_hidden\n",
        "        logits_list = []\n",
        "\n",
        "        for t in range(dec_tokens.shape[0]):\n",
        "            token_id = dec_tokens[t]\n",
        "            x_t = self.embedding(token_id)\n",
        "\n",
        "            h = torch.tanh(\n",
        "                torch.mv(self.W_h, h) +\n",
        "                torch.mv(self.W_x, x_t) +\n",
        "                self.b\n",
        "            )\n",
        "            logits_t = torch.mv(self.W_out, h) + self.b_out\n",
        "            logits_list.append(logits_t.unsqueeze(0))\n",
        "\n",
        "        return torch.cat(logits_list, dim=0)\n",
        "\n",
        "\n",
        "# -------------------------------------\n",
        "# 3. Example Data: \"I go <EOS>\" -> \"मैं जाता हूँ <EOS>\"\n",
        "# -------------------------------------\n",
        "ENG_VOCAB_SIZE = 3  # I=0, go=1, <EOS>=2\n",
        "HIN_VOCAB_SIZE = 5  # <GO>=0, मैं=1, जाता=2, हूँ=3, <EOS>=4\n",
        "\n",
        "# Map IDs to words for printing\n",
        "HIN_ID2WORD = {\n",
        "    0: \"<GO>\",\n",
        "    1: \"मैं\",\n",
        "    2: \"जाता\",\n",
        "    3: \"हूँ\",\n",
        "    4: \"<EOS>\"\n",
        "}\n",
        "\n",
        "EMBED_SIZE = 1\n",
        "HIDDEN_SIZE = 2\n",
        "\n",
        "encoder = TinyEncoder(ENG_VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE)\n",
        "decoder = TinyDecoder(HIN_VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE)\n",
        "\n",
        "# Source: \"I go <EOS>\" => [0,1,2]\n",
        "encoder_input = torch.tensor([0,1,2])\n",
        "\n",
        "# Decoder target: \"मैं जाता हूँ <EOS>\" => [1,2,3,4]\n",
        "# We'll do teacher forcing in training:\n",
        "decoder_input  = torch.tensor([0,1,2,3])  # <GO>, मैं, जाता, हूँ\n",
        "decoder_target = torch.tensor([1,2,3,4])  #     मैं, जाता, हूँ, <EOS>\n",
        "\n",
        "# ----------------------------------\n",
        "# 4. Training Loop (Cross Entropy)\n",
        "# ----------------------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(list(encoder.parameters()) + list(decoder.parameters()), lr=0.0001)\n",
        "\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 1) Encode\n",
        "    enc_hidden = encoder(encoder_input)  # shape (2,)\n",
        "\n",
        "    # 2) Decode\n",
        "    logits = decoder(decoder_input, enc_hidden)  # (4,5)\n",
        "\n",
        "    # 3) Compute cross-entropy\n",
        "    loss = criterion(logits, decoder_target)\n",
        "\n",
        "    # 4) Backprop + update\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print stats\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss = {loss.item():.4f}\")\n",
        "\n",
        "    # ------------------------------\n",
        "    # Print generated words every 20 epochs\n",
        "    # ------------------------------\n",
        "    if (epoch+1) % 20 == 0:\n",
        "        print(f\"\\n--- Decoding after epoch {epoch+1} ---\")\n",
        "        with torch.no_grad():\n",
        "            # Re-encode\n",
        "            enc_hidden = encoder(encoder_input)\n",
        "\n",
        "            # Start <GO>=0\n",
        "            current_token = torch.tensor(0)\n",
        "            h = enc_hidden.clone()\n",
        "\n",
        "            generated_tokens = []\n",
        "            for _ in range(6):\n",
        "                x_t = decoder.embedding(current_token)\n",
        "                h = torch.tanh(\n",
        "                    torch.mv(decoder.W_h, h) +\n",
        "                    torch.mv(decoder.W_x, x_t) +\n",
        "                    decoder.b\n",
        "                )\n",
        "\n",
        "                logits_t = torch.mv(decoder.W_out, h) + decoder.b_out\n",
        "                next_token = torch.argmax(logits_t).item()\n",
        "                generated_tokens.append(next_token)\n",
        "\n",
        "                if next_token == 4:  # <EOS>\n",
        "                    break\n",
        "                current_token = torch.tensor(next_token)\n",
        "\n",
        "            # Convert IDs to words\n",
        "            generated_words = [HIN_ID2WORD[t] for t in generated_tokens]\n",
        "            print(\"Generated tokens:\", generated_words)\n",
        "        print(\"-----------------------------------\\n\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8PZBBWKRns2",
        "outputId": "3f727a56-416a-4085-cd86-84a99e030280"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/1000, Loss = 1.6091\n",
            "Epoch 10/1000, Loss = 1.6091\n",
            "Epoch 15/1000, Loss = 1.6090\n",
            "Epoch 20/1000, Loss = 1.6090\n",
            "\n",
            "--- Decoding after epoch 20 ---\n",
            "Generated tokens: ['<GO>', '<GO>', '<GO>', '<GO>', '<GO>', '<GO>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 25/1000, Loss = 1.6090\n",
            "Epoch 30/1000, Loss = 1.6090\n",
            "Epoch 35/1000, Loss = 1.6089\n",
            "Epoch 40/1000, Loss = 1.6089\n",
            "\n",
            "--- Decoding after epoch 40 ---\n",
            "Generated tokens: ['<GO>', '<GO>', '<GO>', '<GO>', '<GO>', '<GO>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 45/1000, Loss = 1.6089\n",
            "Epoch 50/1000, Loss = 1.6089\n",
            "Epoch 55/1000, Loss = 1.6088\n",
            "Epoch 60/1000, Loss = 1.6088\n",
            "\n",
            "--- Decoding after epoch 60 ---\n",
            "Generated tokens: ['<GO>', '<GO>', '<GO>', '<GO>', '<GO>', '<GO>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 65/1000, Loss = 1.6088\n",
            "Epoch 70/1000, Loss = 1.6088\n",
            "Epoch 75/1000, Loss = 1.6087\n",
            "Epoch 80/1000, Loss = 1.6087\n",
            "\n",
            "--- Decoding after epoch 80 ---\n",
            "Generated tokens: ['<GO>', '<GO>', '<GO>', '<GO>', '<GO>', '<GO>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 85/1000, Loss = 1.6087\n",
            "Epoch 90/1000, Loss = 1.6086\n",
            "Epoch 95/1000, Loss = 1.6086\n",
            "Epoch 100/1000, Loss = 1.6086\n",
            "\n",
            "--- Decoding after epoch 100 ---\n",
            "Generated tokens: ['<GO>', '<GO>', '<GO>', '<GO>', '<GO>', '<GO>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 105/1000, Loss = 1.6086\n",
            "Epoch 110/1000, Loss = 1.6085\n",
            "Epoch 115/1000, Loss = 1.6085\n",
            "Epoch 120/1000, Loss = 1.6085\n",
            "\n",
            "--- Decoding after epoch 120 ---\n",
            "Generated tokens: ['<GO>', '<GO>', '<GO>', '<GO>', '<GO>', '<GO>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 125/1000, Loss = 1.6085\n",
            "Epoch 130/1000, Loss = 1.6084\n",
            "Epoch 135/1000, Loss = 1.6084\n",
            "Epoch 140/1000, Loss = 1.6084\n",
            "\n",
            "--- Decoding after epoch 140 ---\n",
            "Generated tokens: ['<GO>', '<GO>', '<GO>', '<GO>', '<GO>', '<GO>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 145/1000, Loss = 1.6084\n",
            "Epoch 150/1000, Loss = 1.6083\n",
            "Epoch 155/1000, Loss = 1.6083\n",
            "Epoch 160/1000, Loss = 1.6083\n",
            "\n",
            "--- Decoding after epoch 160 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 165/1000, Loss = 1.6083\n",
            "Epoch 170/1000, Loss = 1.6082\n",
            "Epoch 175/1000, Loss = 1.6082\n",
            "Epoch 180/1000, Loss = 1.6082\n",
            "\n",
            "--- Decoding after epoch 180 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 185/1000, Loss = 1.6082\n",
            "Epoch 190/1000, Loss = 1.6081\n",
            "Epoch 195/1000, Loss = 1.6081\n",
            "Epoch 200/1000, Loss = 1.6081\n",
            "\n",
            "--- Decoding after epoch 200 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 205/1000, Loss = 1.6080\n",
            "Epoch 210/1000, Loss = 1.6080\n",
            "Epoch 215/1000, Loss = 1.6080\n",
            "Epoch 220/1000, Loss = 1.6080\n",
            "\n",
            "--- Decoding after epoch 220 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 225/1000, Loss = 1.6079\n",
            "Epoch 230/1000, Loss = 1.6079\n",
            "Epoch 235/1000, Loss = 1.6079\n",
            "Epoch 240/1000, Loss = 1.6079\n",
            "\n",
            "--- Decoding after epoch 240 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 245/1000, Loss = 1.6078\n",
            "Epoch 250/1000, Loss = 1.6078\n",
            "Epoch 255/1000, Loss = 1.6078\n",
            "Epoch 260/1000, Loss = 1.6078\n",
            "\n",
            "--- Decoding after epoch 260 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 265/1000, Loss = 1.6077\n",
            "Epoch 270/1000, Loss = 1.6077\n",
            "Epoch 275/1000, Loss = 1.6077\n",
            "Epoch 280/1000, Loss = 1.6077\n",
            "\n",
            "--- Decoding after epoch 280 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 285/1000, Loss = 1.6076\n",
            "Epoch 290/1000, Loss = 1.6076\n",
            "Epoch 295/1000, Loss = 1.6076\n",
            "Epoch 300/1000, Loss = 1.6076\n",
            "\n",
            "--- Decoding after epoch 300 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 305/1000, Loss = 1.6075\n",
            "Epoch 310/1000, Loss = 1.6075\n",
            "Epoch 315/1000, Loss = 1.6075\n",
            "Epoch 320/1000, Loss = 1.6075\n",
            "\n",
            "--- Decoding after epoch 320 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 325/1000, Loss = 1.6074\n",
            "Epoch 330/1000, Loss = 1.6074\n",
            "Epoch 335/1000, Loss = 1.6074\n",
            "Epoch 340/1000, Loss = 1.6073\n",
            "\n",
            "--- Decoding after epoch 340 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 345/1000, Loss = 1.6073\n",
            "Epoch 350/1000, Loss = 1.6073\n",
            "Epoch 355/1000, Loss = 1.6073\n",
            "Epoch 360/1000, Loss = 1.6072\n",
            "\n",
            "--- Decoding after epoch 360 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 365/1000, Loss = 1.6072\n",
            "Epoch 370/1000, Loss = 1.6072\n",
            "Epoch 375/1000, Loss = 1.6072\n",
            "Epoch 380/1000, Loss = 1.6071\n",
            "\n",
            "--- Decoding after epoch 380 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 385/1000, Loss = 1.6071\n",
            "Epoch 390/1000, Loss = 1.6071\n",
            "Epoch 395/1000, Loss = 1.6071\n",
            "Epoch 400/1000, Loss = 1.6070\n",
            "\n",
            "--- Decoding after epoch 400 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 405/1000, Loss = 1.6070\n",
            "Epoch 410/1000, Loss = 1.6070\n",
            "Epoch 415/1000, Loss = 1.6070\n",
            "Epoch 420/1000, Loss = 1.6069\n",
            "\n",
            "--- Decoding after epoch 420 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 425/1000, Loss = 1.6069\n",
            "Epoch 430/1000, Loss = 1.6069\n",
            "Epoch 435/1000, Loss = 1.6069\n",
            "Epoch 440/1000, Loss = 1.6068\n",
            "\n",
            "--- Decoding after epoch 440 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 445/1000, Loss = 1.6068\n",
            "Epoch 450/1000, Loss = 1.6068\n",
            "Epoch 455/1000, Loss = 1.6068\n",
            "Epoch 460/1000, Loss = 1.6067\n",
            "\n",
            "--- Decoding after epoch 460 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 465/1000, Loss = 1.6067\n",
            "Epoch 470/1000, Loss = 1.6067\n",
            "Epoch 475/1000, Loss = 1.6067\n",
            "Epoch 480/1000, Loss = 1.6066\n",
            "\n",
            "--- Decoding after epoch 480 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 485/1000, Loss = 1.6066\n",
            "Epoch 490/1000, Loss = 1.6066\n",
            "Epoch 495/1000, Loss = 1.6065\n",
            "Epoch 500/1000, Loss = 1.6065\n",
            "\n",
            "--- Decoding after epoch 500 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 505/1000, Loss = 1.6065\n",
            "Epoch 510/1000, Loss = 1.6065\n",
            "Epoch 515/1000, Loss = 1.6064\n",
            "Epoch 520/1000, Loss = 1.6064\n",
            "\n",
            "--- Decoding after epoch 520 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 525/1000, Loss = 1.6064\n",
            "Epoch 530/1000, Loss = 1.6064\n",
            "Epoch 535/1000, Loss = 1.6063\n",
            "Epoch 540/1000, Loss = 1.6063\n",
            "\n",
            "--- Decoding after epoch 540 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 545/1000, Loss = 1.6063\n",
            "Epoch 550/1000, Loss = 1.6063\n",
            "Epoch 555/1000, Loss = 1.6062\n",
            "Epoch 560/1000, Loss = 1.6062\n",
            "\n",
            "--- Decoding after epoch 560 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 565/1000, Loss = 1.6062\n",
            "Epoch 570/1000, Loss = 1.6062\n",
            "Epoch 575/1000, Loss = 1.6061\n",
            "Epoch 580/1000, Loss = 1.6061\n",
            "\n",
            "--- Decoding after epoch 580 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 585/1000, Loss = 1.6061\n",
            "Epoch 590/1000, Loss = 1.6061\n",
            "Epoch 595/1000, Loss = 1.6060\n",
            "Epoch 600/1000, Loss = 1.6060\n",
            "\n",
            "--- Decoding after epoch 600 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 605/1000, Loss = 1.6060\n",
            "Epoch 610/1000, Loss = 1.6060\n",
            "Epoch 615/1000, Loss = 1.6059\n",
            "Epoch 620/1000, Loss = 1.6059\n",
            "\n",
            "--- Decoding after epoch 620 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 625/1000, Loss = 1.6059\n",
            "Epoch 630/1000, Loss = 1.6059\n",
            "Epoch 635/1000, Loss = 1.6058\n",
            "Epoch 640/1000, Loss = 1.6058\n",
            "\n",
            "--- Decoding after epoch 640 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 645/1000, Loss = 1.6058\n",
            "Epoch 650/1000, Loss = 1.6058\n",
            "Epoch 655/1000, Loss = 1.6057\n",
            "Epoch 660/1000, Loss = 1.6057\n",
            "\n",
            "--- Decoding after epoch 660 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 665/1000, Loss = 1.6057\n",
            "Epoch 670/1000, Loss = 1.6057\n",
            "Epoch 675/1000, Loss = 1.6056\n",
            "Epoch 680/1000, Loss = 1.6056\n",
            "\n",
            "--- Decoding after epoch 680 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 685/1000, Loss = 1.6056\n",
            "Epoch 690/1000, Loss = 1.6056\n",
            "Epoch 695/1000, Loss = 1.6055\n",
            "Epoch 700/1000, Loss = 1.6055\n",
            "\n",
            "--- Decoding after epoch 700 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 705/1000, Loss = 1.6055\n",
            "Epoch 710/1000, Loss = 1.6054\n",
            "Epoch 715/1000, Loss = 1.6054\n",
            "Epoch 720/1000, Loss = 1.6054\n",
            "\n",
            "--- Decoding after epoch 720 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 725/1000, Loss = 1.6054\n",
            "Epoch 730/1000, Loss = 1.6053\n",
            "Epoch 735/1000, Loss = 1.6053\n",
            "Epoch 740/1000, Loss = 1.6053\n",
            "\n",
            "--- Decoding after epoch 740 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 745/1000, Loss = 1.6053\n",
            "Epoch 750/1000, Loss = 1.6052\n",
            "Epoch 755/1000, Loss = 1.6052\n",
            "Epoch 760/1000, Loss = 1.6052\n",
            "\n",
            "--- Decoding after epoch 760 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 765/1000, Loss = 1.6052\n",
            "Epoch 770/1000, Loss = 1.6051\n",
            "Epoch 775/1000, Loss = 1.6051\n",
            "Epoch 780/1000, Loss = 1.6051\n",
            "\n",
            "--- Decoding after epoch 780 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 785/1000, Loss = 1.6051\n",
            "Epoch 790/1000, Loss = 1.6050\n",
            "Epoch 795/1000, Loss = 1.6050\n",
            "Epoch 800/1000, Loss = 1.6050\n",
            "\n",
            "--- Decoding after epoch 800 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 805/1000, Loss = 1.6050\n",
            "Epoch 810/1000, Loss = 1.6049\n",
            "Epoch 815/1000, Loss = 1.6049\n",
            "Epoch 820/1000, Loss = 1.6049\n",
            "\n",
            "--- Decoding after epoch 820 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 825/1000, Loss = 1.6049\n",
            "Epoch 830/1000, Loss = 1.6048\n",
            "Epoch 835/1000, Loss = 1.6048\n",
            "Epoch 840/1000, Loss = 1.6048\n",
            "\n",
            "--- Decoding after epoch 840 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 845/1000, Loss = 1.6048\n",
            "Epoch 850/1000, Loss = 1.6047\n",
            "Epoch 855/1000, Loss = 1.6047\n",
            "Epoch 860/1000, Loss = 1.6047\n",
            "\n",
            "--- Decoding after epoch 860 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 865/1000, Loss = 1.6047\n",
            "Epoch 870/1000, Loss = 1.6046\n",
            "Epoch 875/1000, Loss = 1.6046\n",
            "Epoch 880/1000, Loss = 1.6046\n",
            "\n",
            "--- Decoding after epoch 880 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 885/1000, Loss = 1.6046\n",
            "Epoch 890/1000, Loss = 1.6045\n",
            "Epoch 895/1000, Loss = 1.6045\n",
            "Epoch 900/1000, Loss = 1.6045\n",
            "\n",
            "--- Decoding after epoch 900 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 905/1000, Loss = 1.6045\n",
            "Epoch 910/1000, Loss = 1.6044\n",
            "Epoch 915/1000, Loss = 1.6044\n",
            "Epoch 920/1000, Loss = 1.6044\n",
            "\n",
            "--- Decoding after epoch 920 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 925/1000, Loss = 1.6044\n",
            "Epoch 930/1000, Loss = 1.6043\n",
            "Epoch 935/1000, Loss = 1.6043\n",
            "Epoch 940/1000, Loss = 1.6043\n",
            "\n",
            "--- Decoding after epoch 940 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 945/1000, Loss = 1.6043\n",
            "Epoch 950/1000, Loss = 1.6042\n",
            "Epoch 955/1000, Loss = 1.6042\n",
            "Epoch 960/1000, Loss = 1.6042\n",
            "\n",
            "--- Decoding after epoch 960 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 965/1000, Loss = 1.6042\n",
            "Epoch 970/1000, Loss = 1.6041\n",
            "Epoch 975/1000, Loss = 1.6041\n",
            "Epoch 980/1000, Loss = 1.6041\n",
            "\n",
            "--- Decoding after epoch 980 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 985/1000, Loss = 1.6041\n",
            "Epoch 990/1000, Loss = 1.6040\n",
            "Epoch 995/1000, Loss = 1.6040\n",
            "Epoch 1000/1000, Loss = 1.6040\n",
            "\n",
            "--- Decoding after epoch 1000 ---\n",
            "Generated tokens: ['<EOS>']\n",
            "-----------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: LSTM translator coded from scratch"
      ],
      "metadata": {
        "id": "w0DSx8MZFhG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# --------------------------\n",
        "# 1. A Tiny Manual LSTM Encoder\n",
        "# --------------------------\n",
        "class TinyEncoderLSTM(nn.Module):\n",
        "    def __init__(self, input_vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_vocab_size, embed_size)\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # LSTM parameters for the encoder\n",
        "        # Input gate parameters\n",
        "        self.W_i = nn.Parameter(torch.randn(hidden_size, embed_size) )\n",
        "        self.U_i = nn.Parameter(torch.randn(hidden_size, hidden_size) )\n",
        "        self.b_i = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        # Forget gate parameters\n",
        "        self.W_f = nn.Parameter(torch.randn(hidden_size, embed_size) )\n",
        "        self.U_f = nn.Parameter(torch.randn(hidden_size, hidden_size) )\n",
        "        self.b_f = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        # Output gate parameters\n",
        "        self.W_o = nn.Parameter(torch.randn(hidden_size, embed_size) )\n",
        "        self.U_o = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
        "        self.b_o = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        # Candidate cell (g) parameters\n",
        "        self.W_g = nn.Parameter(torch.randn(hidden_size, embed_size) )\n",
        "        self.U_g = nn.Parameter(torch.randn(hidden_size, hidden_size) )\n",
        "        self.b_g = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "    def forward(self, src_tokens):\n",
        "        \"\"\"\n",
        "        src_tokens: shape (src_len,)\n",
        "        Returns final hidden state (h) and cell state (c), each of shape (hidden_size,).\n",
        "        \"\"\"\n",
        "        h = torch.zeros(self.hidden_size)\n",
        "        c = torch.zeros(self.hidden_size)\n",
        "\n",
        "        for t in range(src_tokens.shape[0]):\n",
        "            token_id = src_tokens[t]\n",
        "            x_t = self.embedding(token_id)\n",
        "\n",
        "            i_t = torch.sigmoid(torch.mv(self.W_i, x_t) + torch.mv(self.U_i, h) + self.b_i)\n",
        "            f_t = torch.sigmoid(torch.mv(self.W_f, x_t) + torch.mv(self.U_f, h) + self.b_f)\n",
        "            o_t = torch.sigmoid(torch.mv(self.W_o, x_t) + torch.mv(self.U_o, h) + self.b_o)\n",
        "            g_t = torch.tanh(torch.mv(self.W_g, x_t) + torch.mv(self.U_g, h) + self.b_g)\n",
        "\n",
        "            c = f_t * c + i_t * g_t\n",
        "            h = o_t * torch.tanh(c)\n",
        "\n",
        "        return h, c\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# 2. A Tiny Manual LSTM Decoder\n",
        "# -------------------------\n",
        "class TinyDecoderLSTM(nn.Module):\n",
        "    def __init__(self, output_vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(output_vocab_size, embed_size)\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # LSTM parameters for the decoder\n",
        "        self.W_i = nn.Parameter(torch.randn(hidden_size, embed_size) )\n",
        "        self.U_i = nn.Parameter(torch.randn(hidden_size, hidden_size) )\n",
        "        self.b_i = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        self.W_f = nn.Parameter(torch.randn(hidden_size, embed_size) )\n",
        "        self.U_f = nn.Parameter(torch.randn(hidden_size, hidden_size) )\n",
        "        self.b_f = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        self.W_o = nn.Parameter(torch.randn(hidden_size, embed_size) )\n",
        "        self.U_o = nn.Parameter(torch.randn(hidden_size, hidden_size) )\n",
        "        self.b_o = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        self.W_g = nn.Parameter(torch.randn(hidden_size, embed_size) )\n",
        "        self.U_g = nn.Parameter(torch.randn(hidden_size, hidden_size) )\n",
        "        self.b_g = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        # Output projection parameters\n",
        "        self.W_out = nn.Parameter(torch.randn(output_vocab_size, hidden_size) )\n",
        "        self.b_out = nn.Parameter(torch.zeros(output_vocab_size))\n",
        "\n",
        "    def forward(self, dec_tokens, init_hidden, init_cell):\n",
        "        \"\"\"\n",
        "        dec_tokens: shape (dec_len,)\n",
        "        init_hidden: (hidden_size,)\n",
        "        init_cell: (hidden_size,)\n",
        "        Returns logits of shape (dec_len, output_vocab_size)\n",
        "        \"\"\"\n",
        "        h = init_hidden\n",
        "        c = init_cell\n",
        "        logits_list = []\n",
        "\n",
        "        for t in range(dec_tokens.shape[0]):\n",
        "            token_id = dec_tokens[t]\n",
        "            x_t = self.embedding(token_id)\n",
        "\n",
        "            i_t = torch.sigmoid(torch.mv(self.W_i, x_t) + torch.mv(self.U_i, h) + self.b_i)\n",
        "            f_t = torch.sigmoid(torch.mv(self.W_f, x_t) + torch.mv(self.U_f, h) + self.b_f)\n",
        "            o_t = torch.sigmoid(torch.mv(self.W_o, x_t) + torch.mv(self.U_o, h) + self.b_o)\n",
        "            g_t = torch.tanh(torch.mv(self.W_g, x_t) + torch.mv(self.U_g, h) + self.b_g)\n",
        "\n",
        "            c = f_t * c + i_t * g_t\n",
        "            h = o_t * torch.tanh(c)\n",
        "\n",
        "            logits_t = torch.mv(self.W_out, h) + self.b_out\n",
        "            logits_list.append(logits_t.unsqueeze(0))\n",
        "\n",
        "        return torch.cat(logits_list, dim=0)\n",
        "\n",
        "\n",
        "# -------------------------------------\n",
        "# 3. Example Data: \"I go <EOS>\" -> \"मैं जाता हूँ <EOS>\"\n",
        "# -------------------------------------\n",
        "ENG_VOCAB_SIZE = 3  # I=0, go=1, <EOS>=2\n",
        "HIN_VOCAB_SIZE = 5  # <GO>=0, मैं=1, जाता=2, हूँ=3, <EOS>=4\n",
        "\n",
        "# Map IDs to words for printing\n",
        "HIN_ID2WORD = {\n",
        "    0: \"<GO>\",\n",
        "    1: \"मैं\",\n",
        "    2: \"जाता\",\n",
        "    3: \"हूँ\",\n",
        "    4: \"<EOS>\"\n",
        "}\n",
        "\n",
        "EMBED_SIZE = 1\n",
        "HIDDEN_SIZE = 2\n",
        "\n",
        "encoder = TinyEncoderLSTM(ENG_VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE)\n",
        "decoder = TinyDecoderLSTM(HIN_VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE)\n",
        "\n",
        "# Source: \"I go <EOS>\" => [0,1,2]\n",
        "encoder_input = torch.tensor([0, 1, 2])\n",
        "\n",
        "# Decoder target: \"मैं जाता हूँ <EOS>\" => [1,2,3,4]\n",
        "# For teacher forcing:\n",
        "decoder_input  = torch.tensor([0, 1, 2, 3])  # <GO>, मैं, जाता, हूँ\n",
        "decoder_target = torch.tensor([1, 2, 3, 4])  #     मैं, जाता, हूँ, <EOS>\n",
        "\n",
        "# ----------------------------------\n",
        "# 4. Training Loop (Cross Entropy)\n",
        "# ----------------------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(list(encoder.parameters()) + list(decoder.parameters()), lr=0.1)\n",
        "\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 1) Encode\n",
        "    enc_hidden, enc_cell = encoder(encoder_input)  # each is (hidden_size,)\n",
        "\n",
        "    # 2) Decode (teacher forcing)\n",
        "    logits = decoder(decoder_input, enc_hidden, enc_cell)  # shape (dec_len, HIN_VOCAB_SIZE)\n",
        "\n",
        "    # 3) Compute cross-entropy loss\n",
        "    loss = criterion(logits, decoder_target)\n",
        "\n",
        "    # 4) Backpropagation + update\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print training statistics\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss = {loss.item():.4f}\")\n",
        "\n",
        "    # ------------------------------\n",
        "    # Print generated words every 20 epochs\n",
        "    # ------------------------------\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print(f\"\\n--- Decoding after epoch {epoch+1} ---\")\n",
        "        with torch.no_grad():\n",
        "            # Re-encode the source sentence\n",
        "            enc_hidden, enc_cell = encoder(encoder_input)\n",
        "\n",
        "            # Start decoding with the <GO> token (0)\n",
        "            current_token = torch.tensor(0)\n",
        "            h = enc_hidden.clone()\n",
        "            c = enc_cell.clone()\n",
        "\n",
        "            generated_tokens = []\n",
        "            for _ in range(6):\n",
        "                x_t = decoder.embedding(current_token)\n",
        "                i_t = torch.sigmoid(torch.mv(decoder.W_i, x_t) + torch.mv(decoder.U_i, h) + decoder.b_i)\n",
        "                f_t = torch.sigmoid(torch.mv(decoder.W_f, x_t) + torch.mv(decoder.U_f, h) + decoder.b_f)\n",
        "                o_t = torch.sigmoid(torch.mv(decoder.W_o, x_t) + torch.mv(decoder.U_o, h) + decoder.b_o)\n",
        "                g_t = torch.tanh(torch.mv(decoder.W_g, x_t) + torch.mv(decoder.U_g, h) + decoder.b_g)\n",
        "\n",
        "                c = f_t * c + i_t * g_t\n",
        "                h = o_t * torch.tanh(c)\n",
        "\n",
        "                logits_t = torch.mv(decoder.W_out, h) + decoder.b_out\n",
        "                next_token = torch.argmax(logits_t).item()\n",
        "                generated_tokens.append(next_token)\n",
        "\n",
        "                if next_token == 4:  # <EOS>\n",
        "                    break\n",
        "                current_token = torch.tensor(next_token)\n",
        "\n",
        "            # Convert token IDs to words for display\n",
        "            generated_words = [HIN_ID2WORD[t] for t in generated_tokens]\n",
        "            print(\"Generated tokens:\", generated_words)\n",
        "        print(\"-----------------------------------\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPHHQnJoFrPo",
        "outputId": "69c2101e-e958-4dfb-fc06-3b98d840122b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/1000, Loss = 1.5616\n",
            "Epoch 10/1000, Loss = 1.5170\n",
            "Epoch 15/1000, Loss = 1.4804\n",
            "Epoch 20/1000, Loss = 1.4491\n",
            "\n",
            "--- Decoding after epoch 20 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'हूँ', 'हूँ', 'हूँ', 'हूँ']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 25/1000, Loss = 1.4213\n",
            "Epoch 30/1000, Loss = 1.3959\n",
            "Epoch 35/1000, Loss = 1.3722\n",
            "Epoch 40/1000, Loss = 1.3496\n",
            "\n",
            "--- Decoding after epoch 40 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'हूँ', 'हूँ', 'हूँ', 'हूँ']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 45/1000, Loss = 1.3279\n",
            "Epoch 50/1000, Loss = 1.3068\n",
            "Epoch 55/1000, Loss = 1.2861\n",
            "Epoch 60/1000, Loss = 1.2658\n",
            "\n",
            "--- Decoding after epoch 60 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'हूँ', 'हूँ', 'हूँ', 'हूँ']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 65/1000, Loss = 1.2459\n",
            "Epoch 70/1000, Loss = 1.2261\n",
            "Epoch 75/1000, Loss = 1.2066\n",
            "Epoch 80/1000, Loss = 1.1873\n",
            "\n",
            "--- Decoding after epoch 80 ---\n",
            "Generated tokens: ['मैं', 'मैं', 'हूँ', 'हूँ', 'हूँ', 'हूँ']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 85/1000, Loss = 1.1682\n",
            "Epoch 90/1000, Loss = 1.1493\n",
            "Epoch 95/1000, Loss = 1.1307\n",
            "Epoch 100/1000, Loss = 1.1124\n",
            "\n",
            "--- Decoding after epoch 100 ---\n",
            "Generated tokens: ['मैं', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 105/1000, Loss = 1.0945\n",
            "Epoch 110/1000, Loss = 1.0770\n",
            "Epoch 115/1000, Loss = 1.0599\n",
            "Epoch 120/1000, Loss = 1.0433\n",
            "\n",
            "--- Decoding after epoch 120 ---\n",
            "Generated tokens: ['मैं', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 125/1000, Loss = 1.0272\n",
            "Epoch 130/1000, Loss = 1.0116\n",
            "Epoch 135/1000, Loss = 0.9965\n",
            "Epoch 140/1000, Loss = 0.9820\n",
            "\n",
            "--- Decoding after epoch 140 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', 'हूँ', 'हूँ', 'जाता']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 145/1000, Loss = 0.9680\n",
            "Epoch 150/1000, Loss = 0.9546\n",
            "Epoch 155/1000, Loss = 0.9416\n",
            "Epoch 160/1000, Loss = 0.9292\n",
            "\n",
            "--- Decoding after epoch 160 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', 'हूँ', 'जाता', 'हूँ']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 165/1000, Loss = 0.9172\n",
            "Epoch 170/1000, Loss = 0.9057\n",
            "Epoch 175/1000, Loss = 0.8947\n",
            "Epoch 180/1000, Loss = 0.8840\n",
            "\n",
            "--- Decoding after epoch 180 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', 'हूँ', 'जाता', 'हूँ']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 185/1000, Loss = 0.8737\n",
            "Epoch 190/1000, Loss = 0.8638\n",
            "Epoch 195/1000, Loss = 0.8542\n",
            "Epoch 200/1000, Loss = 0.8450\n",
            "\n",
            "--- Decoding after epoch 200 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 205/1000, Loss = 0.8361\n",
            "Epoch 210/1000, Loss = 0.8274\n",
            "Epoch 215/1000, Loss = 0.8190\n",
            "Epoch 220/1000, Loss = 0.8109\n",
            "\n",
            "--- Decoding after epoch 220 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 225/1000, Loss = 0.8031\n",
            "Epoch 230/1000, Loss = 0.7954\n",
            "Epoch 235/1000, Loss = 0.7880\n",
            "Epoch 240/1000, Loss = 0.7807\n",
            "\n",
            "--- Decoding after epoch 240 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 245/1000, Loss = 0.7737\n",
            "Epoch 250/1000, Loss = 0.7669\n",
            "Epoch 255/1000, Loss = 0.7602\n",
            "Epoch 260/1000, Loss = 0.7537\n",
            "\n",
            "--- Decoding after epoch 260 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 265/1000, Loss = 0.7473\n",
            "Epoch 270/1000, Loss = 0.7411\n",
            "Epoch 275/1000, Loss = 0.7350\n",
            "Epoch 280/1000, Loss = 0.7290\n",
            "\n",
            "--- Decoding after epoch 280 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 285/1000, Loss = 0.7232\n",
            "Epoch 290/1000, Loss = 0.7175\n",
            "Epoch 295/1000, Loss = 0.7118\n",
            "Epoch 300/1000, Loss = 0.7063\n",
            "\n",
            "--- Decoding after epoch 300 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 305/1000, Loss = 0.7009\n",
            "Epoch 310/1000, Loss = 0.6955\n",
            "Epoch 315/1000, Loss = 0.6903\n",
            "Epoch 320/1000, Loss = 0.6851\n",
            "\n",
            "--- Decoding after epoch 320 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 325/1000, Loss = 0.6799\n",
            "Epoch 330/1000, Loss = 0.6749\n",
            "Epoch 335/1000, Loss = 0.6698\n",
            "Epoch 340/1000, Loss = 0.6649\n",
            "\n",
            "--- Decoding after epoch 340 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 345/1000, Loss = 0.6599\n",
            "Epoch 350/1000, Loss = 0.6550\n",
            "Epoch 355/1000, Loss = 0.6502\n",
            "Epoch 360/1000, Loss = 0.6453\n",
            "\n",
            "--- Decoding after epoch 360 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 365/1000, Loss = 0.6405\n",
            "Epoch 370/1000, Loss = 0.6357\n",
            "Epoch 375/1000, Loss = 0.6308\n",
            "Epoch 380/1000, Loss = 0.6260\n",
            "\n",
            "--- Decoding after epoch 380 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 385/1000, Loss = 0.6212\n",
            "Epoch 390/1000, Loss = 0.6164\n",
            "Epoch 395/1000, Loss = 0.6115\n",
            "Epoch 400/1000, Loss = 0.6066\n",
            "\n",
            "--- Decoding after epoch 400 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 405/1000, Loss = 0.6017\n",
            "Epoch 410/1000, Loss = 0.5968\n",
            "Epoch 415/1000, Loss = 0.5918\n",
            "Epoch 420/1000, Loss = 0.5868\n",
            "\n",
            "--- Decoding after epoch 420 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 425/1000, Loss = 0.5817\n",
            "Epoch 430/1000, Loss = 0.5766\n",
            "Epoch 435/1000, Loss = 0.5714\n",
            "Epoch 440/1000, Loss = 0.5662\n",
            "\n",
            "--- Decoding after epoch 440 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 445/1000, Loss = 0.5609\n",
            "Epoch 450/1000, Loss = 0.5555\n",
            "Epoch 455/1000, Loss = 0.5501\n",
            "Epoch 460/1000, Loss = 0.5446\n",
            "\n",
            "--- Decoding after epoch 460 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 465/1000, Loss = 0.5390\n",
            "Epoch 470/1000, Loss = 0.5334\n",
            "Epoch 475/1000, Loss = 0.5277\n",
            "Epoch 480/1000, Loss = 0.5219\n",
            "\n",
            "--- Decoding after epoch 480 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 485/1000, Loss = 0.5161\n",
            "Epoch 490/1000, Loss = 0.5102\n",
            "Epoch 495/1000, Loss = 0.5042\n",
            "Epoch 500/1000, Loss = 0.4982\n",
            "\n",
            "--- Decoding after epoch 500 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 505/1000, Loss = 0.4922\n",
            "Epoch 510/1000, Loss = 0.4861\n",
            "Epoch 515/1000, Loss = 0.4800\n",
            "Epoch 520/1000, Loss = 0.4739\n",
            "\n",
            "--- Decoding after epoch 520 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 525/1000, Loss = 0.4677\n",
            "Epoch 530/1000, Loss = 0.4615\n",
            "Epoch 535/1000, Loss = 0.4554\n",
            "Epoch 540/1000, Loss = 0.4492\n",
            "\n",
            "--- Decoding after epoch 540 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 545/1000, Loss = 0.4430\n",
            "Epoch 550/1000, Loss = 0.4369\n",
            "Epoch 555/1000, Loss = 0.4307\n",
            "Epoch 560/1000, Loss = 0.4246\n",
            "\n",
            "--- Decoding after epoch 560 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 565/1000, Loss = 0.4186\n",
            "Epoch 570/1000, Loss = 0.4125\n",
            "Epoch 575/1000, Loss = 0.4066\n",
            "Epoch 580/1000, Loss = 0.4007\n",
            "\n",
            "--- Decoding after epoch 580 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 585/1000, Loss = 0.3948\n",
            "Epoch 590/1000, Loss = 0.3890\n",
            "Epoch 595/1000, Loss = 0.3833\n",
            "Epoch 600/1000, Loss = 0.3776\n",
            "\n",
            "--- Decoding after epoch 600 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 605/1000, Loss = 0.3720\n",
            "Epoch 610/1000, Loss = 0.3665\n",
            "Epoch 615/1000, Loss = 0.3611\n",
            "Epoch 620/1000, Loss = 0.3558\n",
            "\n",
            "--- Decoding after epoch 620 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 625/1000, Loss = 0.3505\n",
            "Epoch 630/1000, Loss = 0.3453\n",
            "Epoch 635/1000, Loss = 0.3403\n",
            "Epoch 640/1000, Loss = 0.3353\n",
            "\n",
            "--- Decoding after epoch 640 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 645/1000, Loss = 0.3304\n",
            "Epoch 650/1000, Loss = 0.3255\n",
            "Epoch 655/1000, Loss = 0.3208\n",
            "Epoch 660/1000, Loss = 0.3162\n",
            "\n",
            "--- Decoding after epoch 660 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 665/1000, Loss = 0.3116\n",
            "Epoch 670/1000, Loss = 0.3072\n",
            "Epoch 675/1000, Loss = 0.3028\n",
            "Epoch 680/1000, Loss = 0.2985\n",
            "\n",
            "--- Decoding after epoch 680 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 685/1000, Loss = 0.2943\n",
            "Epoch 690/1000, Loss = 0.2902\n",
            "Epoch 695/1000, Loss = 0.2861\n",
            "Epoch 700/1000, Loss = 0.2822\n",
            "\n",
            "--- Decoding after epoch 700 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 705/1000, Loss = 0.2783\n",
            "Epoch 710/1000, Loss = 0.2745\n",
            "Epoch 715/1000, Loss = 0.2708\n",
            "Epoch 720/1000, Loss = 0.2672\n",
            "\n",
            "--- Decoding after epoch 720 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 725/1000, Loss = 0.2636\n",
            "Epoch 730/1000, Loss = 0.2601\n",
            "Epoch 735/1000, Loss = 0.2567\n",
            "Epoch 740/1000, Loss = 0.2534\n",
            "\n",
            "--- Decoding after epoch 740 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 745/1000, Loss = 0.2501\n",
            "Epoch 750/1000, Loss = 0.2469\n",
            "Epoch 755/1000, Loss = 0.2438\n",
            "Epoch 760/1000, Loss = 0.2407\n",
            "\n",
            "--- Decoding after epoch 760 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 765/1000, Loss = 0.2377\n",
            "Epoch 770/1000, Loss = 0.2348\n",
            "Epoch 775/1000, Loss = 0.2319\n",
            "Epoch 780/1000, Loss = 0.2291\n",
            "\n",
            "--- Decoding after epoch 780 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 785/1000, Loss = 0.2263\n",
            "Epoch 790/1000, Loss = 0.2236\n",
            "Epoch 795/1000, Loss = 0.2210\n",
            "Epoch 800/1000, Loss = 0.2184\n",
            "\n",
            "--- Decoding after epoch 800 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 805/1000, Loss = 0.2159\n",
            "Epoch 810/1000, Loss = 0.2134\n",
            "Epoch 815/1000, Loss = 0.2109\n",
            "Epoch 820/1000, Loss = 0.2086\n",
            "\n",
            "--- Decoding after epoch 820 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 825/1000, Loss = 0.2062\n",
            "Epoch 830/1000, Loss = 0.2039\n",
            "Epoch 835/1000, Loss = 0.2017\n",
            "Epoch 840/1000, Loss = 0.1995\n",
            "\n",
            "--- Decoding after epoch 840 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 845/1000, Loss = 0.1973\n",
            "Epoch 850/1000, Loss = 0.1952\n",
            "Epoch 855/1000, Loss = 0.1931\n",
            "Epoch 860/1000, Loss = 0.1911\n",
            "\n",
            "--- Decoding after epoch 860 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 865/1000, Loss = 0.1891\n",
            "Epoch 870/1000, Loss = 0.1872\n",
            "Epoch 875/1000, Loss = 0.1852\n",
            "Epoch 880/1000, Loss = 0.1834\n",
            "\n",
            "--- Decoding after epoch 880 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 885/1000, Loss = 0.1815\n",
            "Epoch 890/1000, Loss = 0.1797\n",
            "Epoch 895/1000, Loss = 0.1779\n",
            "Epoch 900/1000, Loss = 0.1762\n",
            "\n",
            "--- Decoding after epoch 900 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 905/1000, Loss = 0.1745\n",
            "Epoch 910/1000, Loss = 0.1728\n",
            "Epoch 915/1000, Loss = 0.1711\n",
            "Epoch 920/1000, Loss = 0.1695\n",
            "\n",
            "--- Decoding after epoch 920 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 925/1000, Loss = 0.1679\n",
            "Epoch 930/1000, Loss = 0.1664\n",
            "Epoch 935/1000, Loss = 0.1648\n",
            "Epoch 940/1000, Loss = 0.1633\n",
            "\n",
            "--- Decoding after epoch 940 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 945/1000, Loss = 0.1618\n",
            "Epoch 950/1000, Loss = 0.1604\n",
            "Epoch 955/1000, Loss = 0.1589\n",
            "Epoch 960/1000, Loss = 0.1575\n",
            "\n",
            "--- Decoding after epoch 960 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 965/1000, Loss = 0.1561\n",
            "Epoch 970/1000, Loss = 0.1548\n",
            "Epoch 975/1000, Loss = 0.1534\n",
            "Epoch 980/1000, Loss = 0.1521\n",
            "\n",
            "--- Decoding after epoch 980 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n",
            "Epoch 985/1000, Loss = 0.1508\n",
            "Epoch 990/1000, Loss = 0.1495\n",
            "Epoch 995/1000, Loss = 0.1483\n",
            "Epoch 1000/1000, Loss = 0.1470\n",
            "\n",
            "--- Decoding after epoch 1000 ---\n",
            "Generated tokens: ['मैं', 'जाता', 'हूँ', '<EOS>']\n",
            "-----------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: RNN vs LSTM comparison"
      ],
      "metadata": {
        "id": "clxmd547G0PC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# ======================================================\n",
        "# 1. Define a synthetic long-range dependency task\n",
        "# ======================================================\n",
        "# Our vocabularies:\n",
        "# --- Source (English) ---\n",
        "#  0: \"A\"       (the important token)\n",
        "#  1: \"x\"       (a distractor token)\n",
        "#  2: \"<EOS>\"   (end-of-sequence)\n",
        "#\n",
        "# --- Target (Hindi) ---\n",
        "#  0: \"<GO>\"    (start-of-decoding)\n",
        "#  1: \"ए\"      (translation of \"A\")\n",
        "#  2: \"<EOS>\"   (end-of-sequence)\n",
        "\n",
        "ENG_VOCAB_SIZE = 3  # tokens: 0 (\"A\"), 1 (\"x\"), 2 (\"<EOS>\")\n",
        "HIN_VOCAB_SIZE = 3  # tokens: 0 (\"<GO>\"), 1 (\"ए\"), 2 (\"<EOS>\")\n",
        "\n",
        "# For printing decoded Hindi tokens:\n",
        "HIN_ID2WORD = {0: \"<GO>\", 1: \"ए\", 2: \"<EOS>\"}\n",
        "\n",
        "# We will make the source sentence very long by inserting many \"x\" tokens.\n",
        "distractor_length = 500  # Try different lengths (e.g., 5, 50, 100) to see the effect\n",
        "\n",
        "# Construct the source sentence:\n",
        "# It begins with \"A\" (0), then many \"x\" (1), and finally <EOS> (2)\n",
        "encoder_input = torch.tensor([0] + [1] * distractor_length + [2])\n",
        "\n",
        "# The target sentence is fixed: it should translate \"A\" into \"ए\".\n",
        "# (Teacher forcing: decoder input starts with <GO> (0) followed by \"ए\" (1);\n",
        "#  the expected target is \"ए\" (1) then <EOS> (2).)\n",
        "decoder_input = torch.tensor([0, 1])  # <GO>, ए\n",
        "decoder_target = torch.tensor([1, 2])  # ए, <EOS>\n",
        "\n",
        "# ======================================================\n",
        "# 2. Define the Vanilla RNN Encoder and Decoder\n",
        "# ======================================================\n",
        "\n",
        "class TinyEncoderRNN(nn.Module):\n",
        "    def __init__(self, input_vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_vocab_size, embed_size)\n",
        "        self.hidden_size = hidden_size\n",
        "        # Manual RNN parameters (no multiplication by 0.1)\n",
        "        self.W_h = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
        "        self.W_x = nn.Parameter(torch.randn(hidden_size, embed_size))\n",
        "        self.b   = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "    def forward(self, src_tokens):\n",
        "        h = torch.zeros(self.hidden_size)\n",
        "        for t in range(src_tokens.size(0)):\n",
        "            token_id = src_tokens[t]\n",
        "            x_t = self.embedding(token_id)\n",
        "            h = torch.tanh(torch.mv(self.W_h, h) + torch.mv(self.W_x, x_t) + self.b)\n",
        "        return h\n",
        "\n",
        "class TinyDecoderRNN(nn.Module):\n",
        "    def __init__(self, output_vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(output_vocab_size, embed_size)\n",
        "        self.hidden_size = hidden_size\n",
        "        # Manual RNN parameters for decoding\n",
        "        self.W_h = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
        "        self.W_x = nn.Parameter(torch.randn(hidden_size, embed_size))\n",
        "        self.b   = nn.Parameter(torch.zeros(hidden_size))\n",
        "        # Output projection\n",
        "        self.W_out = nn.Parameter(torch.randn(output_vocab_size, hidden_size))\n",
        "        self.b_out = nn.Parameter(torch.zeros(output_vocab_size))\n",
        "\n",
        "    def forward(self, dec_tokens, init_hidden):\n",
        "        h = init_hidden\n",
        "        logits_list = []\n",
        "        for t in range(dec_tokens.size(0)):\n",
        "            token_id = dec_tokens[t]\n",
        "            x_t = self.embedding(token_id)\n",
        "            h = torch.tanh(torch.mv(self.W_h, h) + torch.mv(self.W_x, x_t) + self.b)\n",
        "            logits_t = torch.mv(self.W_out, h) + self.b_out\n",
        "            logits_list.append(logits_t.unsqueeze(0))\n",
        "        return torch.cat(logits_list, dim=0)\n",
        "\n",
        "# ======================================================\n",
        "# 3. Define the LSTM Encoder and Decoder (manual LSTM cell)\n",
        "# ======================================================\n",
        "\n",
        "class TinyEncoderLSTM(nn.Module):\n",
        "    def __init__(self, input_vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_vocab_size, embed_size)\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # LSTM cell parameters (without multiplication by 0.1)\n",
        "        self.W_i = nn.Parameter(torch.randn(hidden_size, embed_size))\n",
        "        self.U_i = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
        "        self.b_i = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        self.W_f = nn.Parameter(torch.randn(hidden_size, embed_size))\n",
        "        self.U_f = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
        "        self.b_f = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        self.W_o = nn.Parameter(torch.randn(hidden_size, embed_size))\n",
        "        self.U_o = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
        "        self.b_o = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        self.W_g = nn.Parameter(torch.randn(hidden_size, embed_size))\n",
        "        self.U_g = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
        "        self.b_g = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "    def forward(self, src_tokens):\n",
        "        h = torch.zeros(self.hidden_size)\n",
        "        c = torch.zeros(self.hidden_size)\n",
        "        for t in range(src_tokens.size(0)):\n",
        "            token_id = src_tokens[t]\n",
        "            x_t = self.embedding(token_id)\n",
        "            i_t = torch.sigmoid(torch.mv(self.W_i, x_t) + torch.mv(self.U_i, h) + self.b_i)\n",
        "            f_t = torch.sigmoid(torch.mv(self.W_f, x_t) + torch.mv(self.U_f, h) + self.b_f)\n",
        "            o_t = torch.sigmoid(torch.mv(self.W_o, x_t) + torch.mv(self.U_o, h) + self.b_o)\n",
        "            g_t = torch.tanh(torch.mv(self.W_g, x_t) + torch.mv(self.U_g, h) + self.b_g)\n",
        "            c = f_t * c + i_t * g_t\n",
        "            h = o_t * torch.tanh(c)\n",
        "        return h, c\n",
        "\n",
        "class TinyDecoderLSTM(nn.Module):\n",
        "    def __init__(self, output_vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(output_vocab_size, embed_size)\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.W_i = nn.Parameter(torch.randn(hidden_size, embed_size))\n",
        "        self.U_i = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
        "        self.b_i = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        self.W_f = nn.Parameter(torch.randn(hidden_size, embed_size))\n",
        "        self.U_f = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
        "        self.b_f = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        self.W_o = nn.Parameter(torch.randn(hidden_size, embed_size))\n",
        "        self.U_o = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
        "        self.b_o = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        self.W_g = nn.Parameter(torch.randn(hidden_size, embed_size))\n",
        "        self.U_g = nn.Parameter(torch.randn(hidden_size, hidden_size))\n",
        "        self.b_g = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "        self.W_out = nn.Parameter(torch.randn(output_vocab_size, hidden_size))\n",
        "        self.b_out = nn.Parameter(torch.zeros(output_vocab_size))\n",
        "\n",
        "    def forward(self, dec_tokens, init_hidden, init_cell):\n",
        "        h = init_hidden\n",
        "        c = init_cell\n",
        "        logits_list = []\n",
        "        for t in range(dec_tokens.size(0)):\n",
        "            token_id = dec_tokens[t]\n",
        "            x_t = self.embedding(token_id)\n",
        "            i_t = torch.sigmoid(torch.mv(self.W_i, x_t) + torch.mv(self.U_i, h) + self.b_i)\n",
        "            f_t = torch.sigmoid(torch.mv(self.W_f, x_t) + torch.mv(self.U_f, h) + self.b_f)\n",
        "            o_t = torch.sigmoid(torch.mv(self.W_o, x_t) + torch.mv(self.U_o, h) + self.b_o)\n",
        "            g_t = torch.tanh(torch.mv(self.W_g, x_t) + torch.mv(self.U_g, h) + self.b_g)\n",
        "            c = f_t * c + i_t * g_t\n",
        "            h = o_t * torch.tanh(c)\n",
        "            logits_t = torch.mv(self.W_out, h) + self.b_out\n",
        "            logits_list.append(logits_t.unsqueeze(0))\n",
        "        return torch.cat(logits_list, dim=0)\n",
        "\n",
        "# ======================================================\n",
        "# 4. Training the models on the synthetic task\n",
        "# ======================================================\n",
        "# Hyperparameters\n",
        "EMBED_SIZE = 4\n",
        "HIDDEN_SIZE = 8\n",
        "num_epochs = 3000\n",
        "learning_rate = 0.001  # step size (learning rate) set to 0.1\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# ----- Train Vanilla RNN Model -----\n",
        "encoder_rnn = TinyEncoderRNN(ENG_VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE)\n",
        "decoder_rnn = TinyDecoderRNN(HIN_VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE)\n",
        "optimizer_rnn = optim.Adam(list(encoder_rnn.parameters()) + list(decoder_rnn.parameters()), lr=learning_rate)\n",
        "\n",
        "print(\"Training Vanilla RNN model on the long-range task...\")\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer_rnn.zero_grad()\n",
        "    enc_hidden = encoder_rnn(encoder_input)\n",
        "    logits = decoder_rnn(decoder_input, enc_hidden)\n",
        "    loss = criterion(logits, decoder_target)\n",
        "    loss.backward()\n",
        "    optimizer_rnn.step()\n",
        "    if (epoch + 1) % 500 == 0:\n",
        "        print(f\"RNN Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# ----- Train LSTM Model -----\n",
        "encoder_lstm = TinyEncoderLSTM(ENG_VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE)\n",
        "decoder_lstm = TinyDecoderLSTM(HIN_VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE)\n",
        "optimizer_lstm = optim.Adam(list(encoder_lstm.parameters()) + list(decoder_lstm.parameters()), lr=learning_rate)\n",
        "\n",
        "print(\"\\nTraining LSTM model on the long-range task...\")\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer_lstm.zero_grad()\n",
        "    enc_hidden, enc_cell = encoder_lstm(encoder_input)\n",
        "    logits = decoder_lstm(decoder_input, enc_hidden, enc_cell)\n",
        "    loss = criterion(logits, decoder_target)\n",
        "    loss.backward()\n",
        "    optimizer_lstm.step()\n",
        "    if (epoch + 1) % 500 == 0:\n",
        "        print(f\"LSTM Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# ======================================================\n",
        "# 5. Define simple decoding functions (greedy decoding)\n",
        "# ======================================================\n",
        "def decode_rnn(encoder, decoder, encoder_input):\n",
        "    with torch.no_grad():\n",
        "        h = encoder(encoder_input)\n",
        "        current_token = torch.tensor(0)  # <GO> token (0)\n",
        "        generated_tokens = []\n",
        "        for _ in range(5):  # limit maximum decoding length\n",
        "            x_t = decoder.embedding(current_token)\n",
        "            h = torch.tanh(torch.mv(decoder.W_h, h) + torch.mv(decoder.W_x, x_t) + decoder.b)\n",
        "            logits_t = torch.mv(decoder.W_out, h) + decoder.b_out\n",
        "            next_token = torch.argmax(logits_t).item()\n",
        "            generated_tokens.append(next_token)\n",
        "            if next_token == 2:  # <EOS>\n",
        "                break\n",
        "            current_token = torch.tensor(next_token)\n",
        "    return generated_tokens\n",
        "\n",
        "def decode_lstm(encoder, decoder, encoder_input):\n",
        "    with torch.no_grad():\n",
        "        h, c = encoder(encoder_input)\n",
        "        current_token = torch.tensor(0)  # <GO>\n",
        "        generated_tokens = []\n",
        "        for _ in range(5):\n",
        "            x_t = decoder.embedding(current_token)\n",
        "            i_t = torch.sigmoid(torch.mv(decoder.W_i, x_t) + torch.mv(decoder.U_i, h) + decoder.b_i)\n",
        "            f_t = torch.sigmoid(torch.mv(decoder.W_f, x_t) + torch.mv(decoder.U_f, h) + decoder.b_f)\n",
        "            o_t = torch.sigmoid(torch.mv(decoder.W_o, x_t) + torch.mv(decoder.U_o, h) + decoder.b_o)\n",
        "            g_t = torch.tanh(torch.mv(decoder.W_g, x_t) + torch.mv(decoder.U_g, h) + decoder.b_g)\n",
        "            c = f_t * c + i_t * g_t\n",
        "            h = o_t * torch.tanh(c)\n",
        "            logits_t = torch.mv(decoder.W_out, h) + decoder.b_out\n",
        "            next_token = torch.argmax(logits_t).item()\n",
        "            generated_tokens.append(next_token)\n",
        "            if next_token == 2:\n",
        "                break\n",
        "            current_token = torch.tensor(next_token)\n",
        "    return generated_tokens\n",
        "\n",
        "# ======================================================\n",
        "# 6. Compare decoding from both models\n",
        "# ======================================================\n",
        "print(\"\\n--- Decoding with the Vanilla RNN model ---\")\n",
        "rnn_decoded = decode_rnn(encoder_rnn, decoder_rnn, encoder_input)\n",
        "print(\"RNN Decoded tokens (Hindi):\", [HIN_ID2WORD[t] for t in rnn_decoded])\n",
        "\n",
        "print(\"\\n--- Decoding with the LSTM model ---\")\n",
        "lstm_decoded = decode_lstm(encoder_lstm, decoder_lstm, encoder_input)\n",
        "print(\"LSTM Decoded tokens (Hindi):\", [HIN_ID2WORD[t] for t in lstm_decoded])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1hv5SkjG6HY",
        "outputId": "423c7d4a-f66d-4f01-f399-713b37e8c800"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Vanilla RNN model on the long-range task...\n",
            "RNN Epoch 500/3000, Loss: 0.0085\n",
            "RNN Epoch 1000/3000, Loss: 0.0038\n",
            "RNN Epoch 1500/3000, Loss: 0.0022\n",
            "RNN Epoch 2000/3000, Loss: 0.0014\n",
            "RNN Epoch 2500/3000, Loss: 0.0009\n",
            "RNN Epoch 3000/3000, Loss: 0.0006\n",
            "\n",
            "Training LSTM model on the long-range task...\n",
            "LSTM Epoch 500/3000, Loss: 0.0303\n",
            "LSTM Epoch 1000/3000, Loss: 0.0057\n",
            "LSTM Epoch 1500/3000, Loss: 0.0024\n",
            "LSTM Epoch 2000/3000, Loss: 0.0014\n",
            "LSTM Epoch 2500/3000, Loss: 0.0009\n",
            "LSTM Epoch 3000/3000, Loss: 0.0006\n",
            "\n",
            "--- Decoding with the Vanilla RNN model ---\n",
            "RNN Decoded tokens (Hindi): ['ए', '<EOS>']\n",
            "\n",
            "--- Decoding with the LSTM model ---\n",
            "LSTM Decoded tokens (Hindi): ['ए', '<EOS>']\n"
          ]
        }
      ]
    }
  ]
}