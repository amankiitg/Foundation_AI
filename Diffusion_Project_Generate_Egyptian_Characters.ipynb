{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amankiitg/Foundation_AI/blob/main/Diffusion_Project_Generate_Egyptian_Characters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Links to Download\n",
        "\n",
        "Han characters dataset: https://drive.google.com/file/d/1JFU5QRn3T11adAYQtA-Of6-rplfVLyOm/view?usp=sharing\n",
        "\n",
        "NotoSans Font: https://drive.google.com/file/d/16Qv5S6_ZV3DzLkpvU-MEbCq94m5Olw1F/view?usp=sharing"
      ],
      "metadata": {
        "id": "oXjCGObZx5h4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivCQyc3a9Hjc"
      },
      "source": [
        "### Step 1: Generate a Chinese glyph and definition dataset\n",
        "We use the unicode consortium's [unihan database](https://github.com/unicode-org/unihan-database) for this. We can take the kDefinition text file to extract both characters and labels in the form of definitions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKcaGGjKjyrx",
        "outputId": "be2f543c-120f-48b4-d593-2a9af325efc5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.25.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import fitz  # PyMuPDF\n",
        "# import re\n",
        "\n",
        "# # Load your PDF\n",
        "# file_path = \"egyptian.pdf\"  # <-- replace with your actual file path\n",
        "# doc = fitz.open(file_path)\n",
        "\n",
        "# # Extract text from pages 17 to 83 (index 16 to 82)\n",
        "# ocr_text = \"\"\n",
        "# for page_num in range(16, 83):\n",
        "#     page = doc.load_page(page_num)\n",
        "#     ocr_text += page.get_text()\n",
        "\n",
        "# # Regex pattern for extracting code + glyph + name\n",
        "# pattern = re.compile(r\"(\\b1[3-4][0-9A-F]{3,4})\\s+([^\\s])\\s+([A-Z \\-0-9]+.*?)\\s*(?=\\n|$)\")\n",
        "# matches = pattern.findall(ocr_text)\n",
        "\n",
        "# # Format matches\n",
        "# formatted_entries = [f\"{code} {glyph} {name.strip()}\" for code, glyph, name in matches]\n",
        "\n",
        "# # Save to a .txt file\n",
        "# output_path = \"egyptian_hieroglyphs_extracted.txt\"\n",
        "# with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "#     for entry in formatted_entries:\n",
        "#         f.write(entry + \"\\n\")\n",
        "\n",
        "# print(f\"Saved {len(formatted_entries)} entries to {output_path}\")\n"
      ],
      "metadata": {
        "id": "x4IdsHl9cQKc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def extract_region_from_pdf(pdf_path, page_number, coords, dpi=300, save_path='output.png'):\n",
        "    \"\"\"\n",
        "    Extract a region from a PDF page as a high-quality image.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF file.\n",
        "        page_number (int): Page number (0-indexed).\n",
        "        coords (tuple): (x0, y0, x1, y1) in points (1/72 inch).\n",
        "        dpi (int): Resolution for rendering the PDF.\n",
        "        save_path (str): Where to save the extracted image.\n",
        "\n",
        "    Returns:\n",
        "        PIL.Image.Image: Extracted image region.\n",
        "    \"\"\"\n",
        "    output_dir = 'png_matrix'\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "        print(f\"Created output directory: {output_dir}\")\n",
        "\n",
        "    # Open the PDF\n",
        "    doc = fitz.open(pdf_path)\n",
        "    page = doc.load_page(page_number)\n",
        "\n",
        "    # Calculate zoom factor from DPI\n",
        "    zoom = dpi / 72\n",
        "    mat = fitz.Matrix(zoom, zoom)\n",
        "\n",
        "    # Render the page to a high-res image\n",
        "    pix = page.get_pixmap(matrix=mat, alpha=False)\n",
        "\n",
        "    # Convert to PIL Image\n",
        "    image = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
        "\n",
        "    # Scale coordinates for high DPI\n",
        "    x0, y0, x1, y1 = [int(c * zoom) for c in coords]\n",
        "\n",
        "    # Crop and save\n",
        "    cropped = image.crop((x0, y0, x1, y1))\n",
        "    cropped.save(output_dir+'/'+str(page_number)+save_path)\n",
        "\n",
        "    return cropped\n",
        "\n",
        "region_coords = (68, 93, 512, 726)  # x0, y0, x1, y1 in PDF points (1/72\")\n",
        "for p in range(2,16,2):\n",
        "  extract_region_from_pdf('egyptian.pdf', p, region_coords, dpi=300, save_path='cropped_image.png')\n",
        "\n",
        "region_coords = (101, 93, 544, 726)  # x0, y0, x1, y1 in PDF points (1/72\")\n",
        "for p in [1]:\n",
        "  extract_region_from_pdf('egyptian.pdf', p, region_coords, dpi=300, save_path='cropped_image.png')\n",
        "\n",
        "region_coords = (100, 93, 544, 726)  # x0, y0, x1, y1 in PDF points (1/72\")\n",
        "for p in range(3,16,2):\n",
        "  extract_region_from_pdf('egyptian.pdf', p, region_coords, dpi=300, save_path='cropped_image.png')"
      ],
      "metadata": {
        "id": "tYfQKIH5jo9z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da5c10fd-fdfa-4dc9-9856-a2b7c2337448"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created output directory: png_matrix\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def extract_grid_images(df, i, input_image_path, output_folder, grid_size=(16, 16), trim_px=(1, 1, 1, 0)):\n",
        "    \"\"\"\n",
        "    Extract images from a grid and save them as separate files with boundary trimming.\n",
        "\n",
        "    Args:\n",
        "        input_image_path (str): Path to the input image containing the grid\n",
        "        output_folder (str): Folder where extracted images will be saved\n",
        "        grid_size (tuple): Number of rows and columns in the grid (rows, cols)\n",
        "        trim_px (tuple): Pixels to trim from each side (left, top, bottom, right)\n",
        "    \"\"\"\n",
        "    # Create output directory if it doesn't exist\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "        print(f\"Created output directory: {output_folder}\")\n",
        "\n",
        "    # Load the image\n",
        "    print(f\"Loading image from: {input_image_path}\")\n",
        "    img = Image.open(input_image_path)\n",
        "\n",
        "    # Get image dimensions\n",
        "    img_width, img_height = img.size\n",
        "\n",
        "    # Calculate the size of each cell\n",
        "    cell_width = img_width / grid_size[1]\n",
        "    cell_height = img_height / grid_size[0]\n",
        "\n",
        "    # print(f\"Image dimensions: {img_width}x{img_height}\")\n",
        "    # print(f\"Cell dimensions: {cell_width}x{cell_height}\")\n",
        "\n",
        "    # Extract and save each cell\n",
        "    total_cells = grid_size[0] * grid_size[1]\n",
        "    saved_count = 0\n",
        "\n",
        "    for row in range(grid_size[0]):\n",
        "        for col in range(grid_size[1]):\n",
        "            # Calculate coordinates with boundary trimming\n",
        "            left = col * cell_width + trim_px[0]\n",
        "            upper = row * cell_height + trim_px[1]\n",
        "            right = (col + 1) * cell_width - trim_px[3]\n",
        "            lower = (row + 1) * cell_height - trim_px[2]\n",
        "\n",
        "            # print((left, upper, right, lower))\n",
        "            # Crop the cell\n",
        "            cell = img.crop((left, upper, right, lower))\n",
        "\n",
        "            # Generate filename with row and column info\n",
        "            # Using hex codes from the image (13D60 + row*16 + col)\n",
        "            hex_code = format(0x13D60 + row * 16 + col, 'X')\n",
        "            filename = str(i)+str(hex_code)+\".png\"\n",
        "            df.loc[len(df.index)] = [str(i)+str(hex_code), filename]\n",
        "            filepath = os.path.join(output_folder, filename)\n",
        "\n",
        "            # Save the cell\n",
        "            cell = cell.resize((128, 128), Image.Resampling.LANCZOS)\n",
        "            cell.save(filepath)\n",
        "            saved_count += 1\n",
        "\n",
        "            # Print progress\n",
        "            # if saved_count % 16 == 0:\n",
        "            #     print(f\"Progress: {saved_count}/{total_cells} images saved\")\n",
        "\n",
        "    print(f\"Extraction complete! {saved_count} images saved to {output_folder}\")\n",
        "\n",
        "df = pd.DataFrame(columns=['Unicode','filename'])\n",
        "# for i in range(1,2):\n",
        "#   input_image = \"png_matrix/\"+str(i)+\"cropped_image.png\"  # Path to your grid image\n",
        "#   output_folder = \"data\"#+str(i)  # Output folder name\n",
        "#   extract_grid_images(df, i, input_image, output_folder, trim_px=(3, 3, 30, 3))\n",
        "\n",
        "# Full usage\n",
        "for i in range(1,11):\n",
        "  input_image = \"png_matrix/\"+str(i)+\"cropped_image.png\"  # Path to your grid image\n",
        "  output_folder = \"data\"#+str(i)  # Output folder name\n",
        "  extract_grid_images(df, i, input_image, output_folder, trim_px=(3, 3, 30, 3))\n",
        "\n",
        "for i in range(11,16):\n",
        "  input_image = \"png_matrix/\"+str(i)+\"cropped_image.png\"  # Path to your grid image\n",
        "  output_folder = \"data\"#+str(i)  # Output folder name\n",
        "  extract_grid_images(df, i, input_image, output_folder, grid_size=(16, 15), trim_px=(3, 3, 30, 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MV9eLt_IZAI3",
        "outputId": "426eea7d-fc50-41d5-c47f-50e1e47fb17a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created output directory: data\n",
            "Loading image from: png_matrix/1cropped_image.png\n",
            "Extraction complete! 256 images saved to data\n",
            "Loading image from: png_matrix/2cropped_image.png\n",
            "Extraction complete! 256 images saved to data\n",
            "Loading image from: png_matrix/3cropped_image.png\n",
            "Extraction complete! 256 images saved to data\n",
            "Loading image from: png_matrix/4cropped_image.png\n",
            "Extraction complete! 256 images saved to data\n",
            "Loading image from: png_matrix/5cropped_image.png\n",
            "Extraction complete! 256 images saved to data\n",
            "Loading image from: png_matrix/6cropped_image.png\n",
            "Extraction complete! 256 images saved to data\n",
            "Loading image from: png_matrix/7cropped_image.png\n",
            "Extraction complete! 256 images saved to data\n",
            "Loading image from: png_matrix/8cropped_image.png\n",
            "Extraction complete! 256 images saved to data\n",
            "Loading image from: png_matrix/9cropped_image.png\n",
            "Extraction complete! 256 images saved to data\n",
            "Loading image from: png_matrix/10cropped_image.png\n",
            "Extraction complete! 256 images saved to data\n",
            "Loading image from: png_matrix/11cropped_image.png\n",
            "Extraction complete! 240 images saved to data\n",
            "Loading image from: png_matrix/12cropped_image.png\n",
            "Extraction complete! 240 images saved to data\n",
            "Loading image from: png_matrix/13cropped_image.png\n",
            "Extraction complete! 240 images saved to data\n",
            "Loading image from: png_matrix/14cropped_image.png\n",
            "Extraction complete! 240 images saved to data\n",
            "Loading image from: png_matrix/15cropped_image.png\n",
            "Extraction complete! 240 images saved to data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ay0gPqPcZfZa",
        "outputId": "23b3acd6-b2a8-4578-8b10-1b1e180b8da5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Unicode    filename\n",
              "0  113D60  113D60.png\n",
              "1  113D61  113D61.png\n",
              "2  113D62  113D62.png\n",
              "3  113D63  113D63.png\n",
              "4  113D64  113D64.png"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-337905c9-8b1f-4b46-8e3f-a7799fa9677c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unicode</th>\n",
              "      <th>filename</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>113D60</td>\n",
              "      <td>113D60.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>113D61</td>\n",
              "      <td>113D61.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>113D62</td>\n",
              "      <td>113D62.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>113D63</td>\n",
              "      <td>113D63.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>113D64</td>\n",
              "      <td>113D64.png</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-337905c9-8b1f-4b46-8e3f-a7799fa9677c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-337905c9-8b1f-4b46-8e3f-a7799fa9677c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-337905c9-8b1f-4b46-8e3f-a7799fa9677c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-82610002-69e1-482a-b41f-ddcd05de80f8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-82610002-69e1-482a-b41f-ddcd05de80f8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-82610002-69e1-482a-b41f-ddcd05de80f8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 3760,\n  \"fields\": [\n    {\n      \"column\": \"Unicode\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3760,\n        \"samples\": [\n          \"713D71\",\n          \"1213E26\",\n          \"113E3C\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"filename\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3760,\n        \"samples\": [\n          \"713D71.png\",\n          \"1213E26.png\",\n          \"113E3C.png\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h_3r5WI9Hjh"
      },
      "source": [
        "### Step 2: Create the dataset of images\n",
        "\n",
        "(1) Take a DataFrame df (like the one you previously created from Unihan-kDefinition.txt)\n",
        "\n",
        "(2) Extract Unicode characters and render them as images using the specified font style (e.g., 128x128 PNGs).\n",
        "\n",
        "(3) Save them into a directory using their Unicode code points as filenames."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bajo7BTW9Hji"
      },
      "source": [
        "Generate metadata.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Convert to JSON Lines format\n",
        "# jsonl_string_with_splits = df.to_json(orient='records', lines=True)\n",
        "\n",
        "# # Write to metadata.jsonl file\n",
        "# with open('metadata.jsonl', 'w') as file:\n",
        "#     file.write(jsonl_string_with_splits)"
      ],
      "metadata": {
        "id": "nsDrxneWZdOz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Converts the DataFrame to JSON Lines format (.jsonl)\n",
        "\n",
        "Each line is a separate JSON object, which is ideal for training datasets in ML frameworks like Hugging Face, PyTorch, etc."
      ],
      "metadata": {
        "id": "wh_okz-EWZuQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Define training model configuration"
      ],
      "metadata": {
        "id": "QPU7KJcNXE6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    image_size = 128  # assumes images are square\n",
        "    train_batch_size = 32\n",
        "    eval_batch_size = 32\n",
        "    num_epochs = 30\n",
        "    gradient_accumulation_steps = 1\n",
        "    learning_rate = 1e-4\n",
        "    lr_warmup_steps = 500\n",
        "    save_image_epochs = 1\n",
        "    save_model_epochs = 30\n",
        "    mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
        "    output_dir = \"glyffuser-unconditional\"  # the model name\n",
        "    overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
        "    seed = 0\n",
        "    dataset_name=\"data128\"\n",
        "\n",
        "config = TrainingConfig()"
      ],
      "metadata": {
        "id": "RHeGh7XU-2m2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Create Local Dataset Class and helper functions"
      ],
      "metadata": {
        "id": "PD7QF9PjXKpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "from datasets import Dataset\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "import torchvision.transforms as T\n",
        "import torch.nn.functional as F\n",
        "from diffusers import UNet2DModel, DDPMScheduler, DPMSolverMultistepScheduler\n",
        "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
        "from diffusers import DDPMPipeline\n",
        "from accelerate import Accelerator\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "\n",
        "def normalize_neg_one_to_one(img):\n",
        "    return img * 2 - 1\n",
        "\n",
        "# UPDATED LocalDataset class\n",
        "class LocalDataset(Dataset):\n",
        "    def __init__(self, folder, image_size, exts=['png']):\n",
        "        super().__init__()\n",
        "        self.folder = folder\n",
        "        self.image_size = image_size\n",
        "\n",
        "        # Match extensions case-insensitively\n",
        "        self.paths = []\n",
        "        for ext in exts:\n",
        "            self.paths.extend([\n",
        "                p for p in Path(folder).rglob(f'*.{ext}')\n",
        "                if p.suffix.lower() == f'.{ext.lower()}'\n",
        "            ])\n",
        "\n",
        "        assert len(self.paths) > 0, f\"No images found in {folder}. Check path and extensions.\"\n",
        "\n",
        "        self.transform = T.Compose([\n",
        "            T.Resize((image_size, image_size)),\n",
        "            T.ToTensor(),\n",
        "            T.Lambda(normalize_neg_one_to_one),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path = self.paths[index]\n",
        "        img = Image.open(path).convert('L')  # Grayscale\n",
        "        return self.transform(img)\n",
        "\n",
        "def make_grid(images, rows, cols):\n",
        "    # Helper function for making a grid of images\n",
        "    w, h = images[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    for i, image in enumerate(images):\n",
        "        grid.paste(image, box=(i%cols*w, i//cols*h))\n",
        "    return grid\n",
        "\n",
        "def evaluate(config, epoch, pipeline):\n",
        "    # Sample from the model and save the images in a grid\n",
        "    images = pipeline(\n",
        "        batch_size=config.eval_batch_size,\n",
        "        generator=torch.Generator(device='cpu').manual_seed(config.seed),\n",
        "        num_inference_steps=50\n",
        "    ).images\n",
        "\n",
        "    image_grid = make_grid(images, rows=4, cols=4)\n",
        "\n",
        "    test_dir = os.path.join(config.output_dir, \"samples\")\n",
        "    os.makedirs(test_dir, exist_ok=True)\n",
        "    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RV7ieh1-8fH",
        "outputId": "d67a42a4-1d80-430e-b733-994fda796dd0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Define model training loop"
      ],
      "metadata": {
        "id": "lM5kUcgvXST4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n",
        "    accelerator = Accelerator(\n",
        "        mixed_precision=config.mixed_precision,\n",
        "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
        "        log_with=\"tensorboard\",\n",
        "        project_dir=os.path.join(config.output_dir, \"logs\")\n",
        "    )\n",
        "\n",
        "    if accelerator.is_main_process:\n",
        "        if config.output_dir is not None:\n",
        "            os.makedirs(config.output_dir, exist_ok=True)\n",
        "        accelerator.init_trackers(\"train_example\")\n",
        "\n",
        "    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "        model, optimizer, train_dataloader, lr_scheduler\n",
        "    )\n",
        "\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(config.num_epochs):\n",
        "        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
        "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            clean_images = batch\n",
        "            noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
        "            bs = clean_images.shape[0]\n",
        "            timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bs,), device=clean_images.device).long()\n",
        "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
        "\n",
        "            with accelerator.accumulate(model):\n",
        "                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
        "                loss = F.mse_loss(noise_pred, noise)\n",
        "                accelerator.backward(loss)\n",
        "                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                lr_scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            progress_bar.update(1)\n",
        "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
        "            progress_bar.set_postfix(**logs)\n",
        "            accelerator.log(logs, step=global_step)\n",
        "            global_step += 1\n",
        "\n",
        "        if accelerator.is_main_process:\n",
        "            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
        "                pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=inference_scheduler)\n",
        "                evaluate(config, epoch, pipeline)\n",
        "\n",
        "            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
        "                pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=inference_scheduler)\n",
        "                save_dir = os.path.join(config.output_dir, f\"epoch{epoch}\")\n",
        "                pipeline.save_pretrained(save_dir)"
      ],
      "metadata": {
        "id": "u4okkjY2XUYc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7: Load dataset"
      ],
      "metadata": {
        "id": "FMh_fGstXXbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data source\n",
        "dataset = LocalDataset(\"data\", image_size=config.image_size)\n",
        "train_dataloader = DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "Cb9XLgJn_L2e"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 8: Define UNet Model"
      ],
      "metadata": {
        "id": "8txNXtRcXcMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model\n",
        "model = UNet2DModel(\n",
        "    sample_size=config.image_size,  # the target image resolution\n",
        "    in_channels=1,  # the number of input channels\n",
        "    out_channels=1,  # the number of output channels\n",
        "    layers_per_block=1,  # how many ResNet layers to use per UNet block\n",
        "    block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channels for each UNet block\n",
        "    down_block_types=(\n",
        "        \"DownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "        \"AttnDownBlock2D\",\n",
        "        \"DownBlock2D\",\n",
        "    ),\n",
        "    up_block_types=(\n",
        "        \"UpBlock2D\",\n",
        "        \"AttnUpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "0LZ_aRt-Xd9c"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 9: Define optimizers and schedulers"
      ],
      "metadata": {
        "id": "KfmGAQdaXe06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
        "inference_scheduler = DPMSolverMultistepScheduler()\n",
        "optimizer = AdamW(model.parameters(), lr=config.learning_rate)\n",
        "lr_scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=config.lr_warmup_steps,\n",
        "    num_training_steps=(len(train_dataloader) * config.num_epochs),\n",
        ")"
      ],
      "metadata": {
        "id": "AbfMLI6RXjLV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 10: Run training on the model!"
      ],
      "metadata": {
        "id": "jyksgzGeXkEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image, ImageDraw, ImageFont"
      ],
      "metadata": {
        "id": "8Uy3iXo3aDIo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import notebook_launcher\n",
        "\n",
        "args = (config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)\n",
        "notebook_launcher(train_loop, args, num_processes=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124,
          "referenced_widgets": [
            "ea10cc9f6d2040c9af2602b94e9c5c01",
            "251ffc52b2b545b7bb461526381d55bc",
            "3422626211b9428bbfd8f2972a570237",
            "4e7abb02d5794aacb5f5e255b8657494",
            "cf6a1aa841be449398058315299980dc",
            "5be38e5ab9ba440bb0da1440bf3a38b5",
            "d38aeb538b3f4f9b89344f4670d8a382",
            "d6937ecc85f34f67aae130cd2b57e802",
            "3853edbcd3f64d33b617cec88c8a2e3f",
            "c24996ce10f64149a9a6b3ff70aa19dc",
            "5a4d3ee8f5e5456db543baa6303e6d83"
          ]
        },
        "id": "UKCI5mgrBGhT",
        "outputId": "27578c56-f230-4467-a971-082990bdf5fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Launching training on one GPU.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/118 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea10cc9f6d2040c9af2602b94e9c5c01"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/diffusers/configuration_utils.py:140: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'DDPMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'DDPMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.\n",
            "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 11: Sample from trained diffusion model\n",
        "\n",
        "Loads a pretrained DDPMPipeline from epoch99 of a local model directory (likely trained on glyphs or characters).\n",
        "\n",
        "Moves the model to GPU.\n",
        "\n",
        "Sets the scheduler to DPMSolverMultistepScheduler, which controls the denoising steps during sampling.\n",
        "\n",
        "Generates 16 images in one batch.\n",
        "\n",
        "Uses a fixed random seed (config.seed) for reproducibility.\n",
        "\n",
        "Uses 50 diffusion steps to sample clean images from noise.\n",
        "\n",
        "Arranges the 16 images into a 4x4 grid.\n",
        "\n",
        "Creates a samples folder inside your config.output_dir.\n",
        "\n",
        "Saves the resulting image grid as samples.png."
      ],
      "metadata": {
        "id": "mLcitA0uXnpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"glyffuser-unconditional/epoch29\"  # Path to the specific epoch model directory# Path to the model directory\n",
        "pipeline = DDPMPipeline.from_pretrained(model_path).to(\"cuda\")\n",
        "pipeline.scheduler = DPMSolverMultistepScheduler()\n",
        "\n",
        "# Sample from the model and save the images in a grid\n",
        "images = pipeline(\n",
        "    batch_size=16,\n",
        "    generator=torch.Generator(device='cuda').manual_seed(config.seed), # Generator can be on GPU here\n",
        "    num_inference_steps=50\n",
        ").images\n",
        "\n",
        "# Make a grid out of the inverted images\n",
        "image_grid = make_grid(images, rows=4, cols=4)\n",
        "\n",
        "# Save the images\n",
        "test_dir = os.path.join(config.output_dir, \"samples\")\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "image_grid.save(f\"{test_dir}/samples.png\")"
      ],
      "metadata": {
        "id": "T3hJns34BPLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 12: Visualize model generated images"
      ],
      "metadata": {
        "id": "2JeENU5RYGwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import DPMSolverMultistepScheduler, DDPMPipeline\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import torch\n",
        "\n",
        "def make_labeled_grid(images, prompt, steps, font_path=None, font_size=20, margin=10):\n",
        "    assert len(images) == len(steps), \"The number of images must match the number of steps\"\n",
        "\n",
        "    w, h = images[0].size\n",
        "    font = ImageFont.truetype(font_path, font_size) if font_path else ImageFont.load_default()\n",
        "\n",
        "    # Calculate the height of the grid including the margin for text\n",
        "    total_height = h + margin + font_size\n",
        "    total_width = w * len(images)\n",
        "    grid_height = total_height + margin + font_size  # Add extra margin for the prompt\n",
        "    grid = Image.new('RGB', size=(total_width, grid_height), color=(255, 255, 255))\n",
        "    # Draw the text prompt at the top\n",
        "    draw = ImageDraw.Draw(grid)\n",
        "    prompt_text = f\"Prompt: \\\"{prompt}\\\"\"\n",
        "    prompt_width, prompt_height = draw.textbbox((0, 0), prompt_text, font=font)[2:4]\n",
        "    prompt_x = (total_width - prompt_width) / 2\n",
        "    prompt_y = margin / 2\n",
        "    draw.text((prompt_x, prompt_y), prompt_text, fill=\"black\", font=font)\n",
        "\n",
        "    for i, (image, step) in enumerate(zip(images, steps)):\n",
        "        # Calculate position to paste the image\n",
        "        x = i * w\n",
        "        y = margin + font_size\n",
        "\n",
        "        # Paste the image\n",
        "        grid.paste(image, box=(x, y))\n",
        "\n",
        "        # Draw the step text\n",
        "        step_text = f\"Steps: {step}\"\n",
        "        text_width, text_height = draw.textbbox((0, 0), step_text, font=font)[2:4]\n",
        "        text_x = x + (w - text_width) / 2\n",
        "        text_y = y + h + margin / 2 - 8\n",
        "        draw.text((text_x, text_y), step_text, fill=\"black\", font=font)\n",
        "    return grid\n",
        "\n",
        "# Initialize the model pipeline using your local model\n",
        "model_path = \"glyffuser-unconditional/epoch29\"  # Path to your trained model\n",
        "pipeline = DDPMPipeline.from_pretrained(model_path).to(\"cuda\")\n",
        "pipeline.scheduler = DPMSolverMultistepScheduler()\n",
        "\n",
        "# Define the number of steps to visualize\n",
        "num_inference_steps_list = [1, 2, 3, 5, 10, 20, 50]\n",
        "\n",
        "images = []\n",
        "\n",
        "# Generate images for each value in num_inference_steps_list\n",
        "for num_steps in num_inference_steps_list:\n",
        "    generated_images = pipeline(\n",
        "        batch_size=1,\n",
        "        generator=torch.Generator(device='cuda').manual_seed(0),\n",
        "        num_inference_steps=num_steps\n",
        "    ).images\n",
        "    images.append(generated_images[0])  # Append the generated image\n",
        "\n",
        "# Create the labeled grid with a descriptive prompt since this is an unconditional model\n",
        "prompt = \"Unconditional Diffusion Model Output\"\n",
        "image_grid = make_labeled_grid(images, prompt, num_inference_steps_list)\n",
        "\n",
        "# Show the grid\n",
        "from IPython.display import display\n",
        "display(image_grid)\n",
        "\n",
        "# Save the image grid\n",
        "image_grid.save(\"diffusion_steps_visualization30.png\")"
      ],
      "metadata": {
        "id": "pGa7pruCUaTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 13: Create animated visualization"
      ],
      "metadata": {
        "id": "a8l6iv4iYRcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import DPMSolverMultistepScheduler, DDPMPipeline\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import imageio.v2 as imageio\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize the model pipeline using your local model\n",
        "model_path = \"glyffuser-unconditional/epoch29\"  # Path to your trained model\n",
        "pipeline = DDPMPipeline.from_pretrained(model_path).to(\"cuda\")\n",
        "pipeline.scheduler = DPMSolverMultistepScheduler()\n",
        "\n",
        "# Create output directory for frames\n",
        "os.makedirs(\"animation_frames\", exist_ok=True)\n",
        "\n",
        "# Set parameters\n",
        "num_inference_steps = 50\n",
        "seed = 42\n",
        "\n",
        "# Use the model's forward process to generate images at each step\n",
        "print(\"Generating denoising frames...\")\n",
        "\n",
        "# Start with pure noise (t=1000)\n",
        "generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
        "\n",
        "# Store all frames\n",
        "frames = []\n",
        "\n",
        "# The correct way to visualize the denoising process is to use the pipeline with\n",
        "# increasing numbers of denoising steps\n",
        "for step in tqdm(range(0, num_inference_steps + 1, 2)):  # Skip some steps for faster generation\n",
        "    if step == 0:\n",
        "        # For the initial noise, just use the pipeline with 1 step\n",
        "        # This will effectively show the noise\n",
        "        current_step = 1\n",
        "    else:\n",
        "        current_step = step\n",
        "\n",
        "    # Generate the image with the current number of denoising steps\n",
        "    image = pipeline(\n",
        "        batch_size=1,\n",
        "        generator=torch.Generator(device=\"cuda\").manual_seed(seed),\n",
        "        num_inference_steps=current_step\n",
        "    ).images[0]\n",
        "\n",
        "    # Save the frame\n",
        "    image.save(os.path.join(\"animation_frames\", f\"frame_{step:03d}.png\"))\n",
        "    frames.append(image)\n",
        "\n",
        "# Create GIF from frames\n",
        "print(\"Creating GIF animation...\")\n",
        "# Ensure all frames have the same size (shouldn't be necessary but just in case)\n",
        "frames_resized = [frame.resize((256, 256)) for frame in frames]\n",
        "\n",
        "# Save as GIF\n",
        "output_gif = \"diffusion_process30.gif\"\n",
        "frames_resized[0].save(\n",
        "    output_gif,\n",
        "    save_all=True,\n",
        "    append_images=frames_resized[1:],\n",
        "    optimize=False,\n",
        "    duration=150,  # milliseconds per frame - slower to see the changes\n",
        "    loop=0  # 0 means loop indefinitely\n",
        ")\n",
        "\n",
        "print(f\"Animation saved to {output_gif}\")\n",
        "\n",
        "# Create a grid showing selected frames\n",
        "def create_process_grid(frames, num_to_show=8):\n",
        "    # Select frames evenly throughout the process\n",
        "    if len(frames) <= num_to_show:\n",
        "        selected_frames = frames\n",
        "    else:\n",
        "        indices = np.linspace(0, len(frames)-1, num_to_show, dtype=int)\n",
        "        selected_frames = [frames[i] for i in indices]\n",
        "\n",
        "    # Resize frames\n",
        "    width, height = 256, 256\n",
        "    selected_frames = [frame.resize((width, height)) for frame in selected_frames]\n",
        "\n",
        "    # Create grid image\n",
        "    cols = min(4, num_to_show)\n",
        "    rows = (num_to_show + cols - 1) // cols\n",
        "\n",
        "    grid = Image.new('RGB', (width * cols, height * rows))\n",
        "\n",
        "    for i, frame in enumerate(selected_frames):\n",
        "        row = i // cols\n",
        "        col = i % cols\n",
        "        grid.paste(frame, (col * width, row * height))\n",
        "\n",
        "    return grid\n",
        "\n",
        "# Create and save the grid\n",
        "grid = create_process_grid(frames)\n",
        "grid.save(\"diffusion_process_grid3.png\")\n",
        "print(\"Process grid saved to diffusion_process_grid3.png\")"
      ],
      "metadata": {
        "id": "LKf-_SwnUvtu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ea10cc9f6d2040c9af2602b94e9c5c01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_251ffc52b2b545b7bb461526381d55bc",
              "IPY_MODEL_3422626211b9428bbfd8f2972a570237",
              "IPY_MODEL_4e7abb02d5794aacb5f5e255b8657494"
            ],
            "layout": "IPY_MODEL_cf6a1aa841be449398058315299980dc"
          }
        },
        "251ffc52b2b545b7bb461526381d55bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5be38e5ab9ba440bb0da1440bf3a38b5",
            "placeholder": "​",
            "style": "IPY_MODEL_d38aeb538b3f4f9b89344f4670d8a382",
            "value": "Epoch 0:  35%"
          }
        },
        "3422626211b9428bbfd8f2972a570237": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6937ecc85f34f67aae130cd2b57e802",
            "max": 118,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3853edbcd3f64d33b617cec88c8a2e3f",
            "value": 41
          }
        },
        "4e7abb02d5794aacb5f5e255b8657494": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c24996ce10f64149a9a6b3ff70aa19dc",
            "placeholder": "​",
            "style": "IPY_MODEL_5a4d3ee8f5e5456db543baa6303e6d83",
            "value": " 41/118 [00:39&lt;01:08,  1.12it/s, loss=0.294, lr=8e-6, step=40]"
          }
        },
        "cf6a1aa841be449398058315299980dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5be38e5ab9ba440bb0da1440bf3a38b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d38aeb538b3f4f9b89344f4670d8a382": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6937ecc85f34f67aae130cd2b57e802": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3853edbcd3f64d33b617cec88c8a2e3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c24996ce10f64149a9a6b3ff70aa19dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a4d3ee8f5e5456db543baa6303e6d83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}